{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cebc0463-1f2f-4eb5-baf0-304f868626e7",
   "metadata": {},
   "source": [
    "### set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c19e1ae9-3a6e-4e8b-a2b9-49b750c01da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math,os, sys,torch,re, numpy as np\n",
    "from types import SimpleNamespace as ns\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e57e186-57d9-43bc-b895-8a18799a2695",
   "metadata": {},
   "source": [
    "### cuda setup for paperspace\n",
    "* pip install --disable-pip-version-check --root-user-action=ignore wurlitzer ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f781ec61-640f-48fb-9635-0cec8e347c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim3 = namedtuple('dim3', ['x','y','z'], defaults=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba474087-88a8-49a3-bab7-daeed41cfe09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dim3(x=2, y=3, z=1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = dim3(2,3)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19312f7f-0fb6-4227-a0f1-2aed7edcc50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.x,d.y,d.z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c9be7bc-3b69-4acf-aa6d-cdf231bf1e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2, linewidth=140)\n",
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32124521-149e-49bd-bc6b-bc6d7c25c3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e6282c6-3d49-401f-a29c-8053da1b8bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utills import show_img,load_cuda, cuda_begin, cdiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "087b4efb-75e0-4251-b0d0-7ad48a8ec683",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext wurlitzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3545bec-b105-4963-89f4-d17e4185323b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/wurlitzer.py:211: RuntimeWarning: Failed to set pipe buffer size: [Errno 1] Operation not permitted\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158ecf44-5ebd-46f4-bf0b-ed32106ee74a",
   "metadata": {},
   "source": [
    "### N.B\n",
    "- the bug was using randn instead of rand, it somehow has a big diffrence even tho the same data to do the same operation (python matmal with tiled matmal(still using python), some how cuda dont give the same bug it handles this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e3ddad1c-5b7f-4e05-9512-7c778a2a2559",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = torch.rand(5120,256)\n",
    "m1s = m1[:4]\n",
    "m2 = torch.rand(256, 5120)\n",
    "m2s = m2[:,:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61194e92-3605-4ecd-9b36-429f3f800440",
   "metadata": {},
   "source": [
    "#### Python 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc47901a-1325-4780-a565-58ede3d55943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blk_kernel2d(f, blocks, threads, *args):\n",
    "    for i0 in range(blocks.y):\n",
    "        for i1 in range(blocks.x):\n",
    "            for j0 in range(threads.y):\n",
    "                for j1 in range(threads.x): f(dim3(i1,i0),dim3(j1,j0), threads, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fd64bdd-848e-4ccf-ba59-b4fdb97b9cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmal_bk(blockIdx, threadIdx, blockDim, m,n, out, h,w,k):\n",
    "    r = blockIdx.y*blockDim.y + threadIdx.y\n",
    "    c = blockIdx.x*blockDim.x + threadIdx.x\n",
    "    \n",
    "    if (r>=h or c>=w): return\n",
    "    o = 0.\n",
    "    for i in range(k): o += m[r*k+i] * n[i*w+c]\n",
    "    out[r*w+c] = o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7e1730f-7dfa-4954-b75d-2ebda18afa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmal2d(m,n):\n",
    "    h,k = m.shape\n",
    "    k2,w = n.shape\n",
    "    assert k2==k, 'Size mismatch !'\n",
    "    output = torch.zeros(h,w, dtype=m.dtype)\n",
    "    tpb = dim3(16,16)\n",
    "    blocks = dim3(cdiv(w, tpb.x), cdiv(h,tpb.y))\n",
    "    blk_kernel2d(matmal_bk, blocks, tpb,\n",
    "                m.flatten(), n.flatten(), output.flatten(),h,w,k)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bb5022e-72f6-4eee-ab25-533e6e7c93f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(matmal2d(m1s,m2s), m1s@m2s).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "600b1dcf-da18-4a4e-b77f-7bf099c48528",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_src = cuda_begin + r'''\n",
    "__global__ void matmal_k(float* m,float* n, float* out, int h, int w, int k) {\n",
    "    int r = blockIdx.y*blockDim.y + threadIdx.y;\n",
    "    int c = blockIdx.x*blockDim.x + threadIdx.x;\n",
    "    \n",
    "    if (r>=h || c>=w) return;\n",
    "    float o = 0;\n",
    "    for(int i=0; i<k; ++i) o += m[r*k+i] * n[i*w+c];\n",
    "    out[r*w+c] = o;\n",
    "}\n",
    "\n",
    "torch::Tensor matmul(torch::Tensor m, torch::Tensor n){\n",
    "    CHECK_INPUT(m);CHECK_INPUT(n);\n",
    "    int h = m.size(0);\n",
    "    int w = n.size(1);\n",
    "    int k = m.size(1);\n",
    "    TORCH_CHECK(k==n.size(0), \"SIZE MISMATCH!\");\n",
    "    auto output = torch::zeros({h,w}, m.options());\n",
    "    \n",
    "    dim3 tpb(16,16);\n",
    "    dim3 blocks(cdiv(w, tpb.x), cdiv(h, tpb.y));\n",
    "    matmal_k<<<blocks, tpb>>>(\n",
    "        m.data_ptr<float>(), n.data_ptr<float>(), output.data_ptr<float>(), h, w, k);\n",
    "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
    "    return output;\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e55b7c77-b04a-4e70-a2e3-b1587f24dfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'matmul'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aff436d1-4d32-41b7-9db6-47f2bea96480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sig(fname,src):\n",
    "    res = re.findall(rf'^(.+\\s+{fname}.*?)\\s*{{?\\s*$', src, re.MULTILINE)\n",
    "    return res[0]+';' if res else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "902735fe-57cb-41a3-8e4a-541bd2b81e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch::Tensor matmul(torch::Tensor m, torch::Tensor n);'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpp_src = get_sig(fname, cuda_src)\n",
    "cpp_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "049d82e4-a289-48fe-8102-d5d8c83e347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "module = load_cuda(cuda_src, cpp_src, [fname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48f06fdf-90e3-4082-b95e-d63b744f7736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/wurlitzer.py:211: RuntimeWarning: Failed to set pipe buffer size: [Errno 1] Operation not permitted\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "m1c,m2c = m1.contiguous().cuda(),m2.contiguous().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d5d84d5-e4a5-4444-a61e-0d93f19f9936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5120, 5120])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.matmul(m1c,m2c).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4c91668-dad2-47c5-a313-9f70601a03ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(module.matmul(m1c,m2c), m1c@m2c).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0236bf97-4fdd-4481-ba46-526730449bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.3 ms ± 242 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 10\n",
    "module.matmul(m1c,m2c)\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9d61cbb1-141d-4d9d-bccd-ef47e7af7bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blk_kernel2D_shrd(f,blocks, threads,sh_sz,*args, **kwargs):\n",
    "    for i0 in range(blocks.y):\n",
    "        for i1 in range(blocks.x):\n",
    "            shared = torch.zeros(sh_sz)\n",
    "            f(dim3(i1,i0),threads, shared,*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "96e45561-bb81-41e0-a26a-74ed05a28b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiled_matmal_blk(blockIdx, blockDim, shared, m, n, out, h, w, k, tw):\n",
    "    shar_sz = tw*tw\n",
    "    ms, ns = shared[:shar_sz],shared[shar_sz:] # dividing the shared mem to ms and ns\n",
    "    \n",
    "    for ph in range(cdiv(k,tw)):\n",
    "        idx = ph*tw # how far starting idx of the next p(output) as we index one tile after another before we reach the coardinates of the p tile\n",
    "        \n",
    "        #fill shared mem\n",
    "        for tr in range(blockDim.y):\n",
    "            for tc in range(blockDim.x):\n",
    "                r = blockIdx.y*blockDim.y + tr # coordinate location within the tile \n",
    "                c = blockIdx.x*blockDim.x + tc\n",
    "                \n",
    "                ms[tr*tw+tc] = m[tc+idx+ r*k] if r<h and idx+tc<k else 0.\n",
    "                ns[tr*tw+tc] = n[(tr+idx)*w+ c] if c<w and idx+tr<k else 0.\n",
    "                \n",
    "            #do dot product\n",
    "            for tr in range(blockDim.y):\n",
    "                for tc in range(blockDim.x):\n",
    "                    r,c = blockIdx.y*blockDim.y +tr, blockIdx.x*blockDim.x +tc\n",
    "                    for i in range(tw):\n",
    "                        if r*w+c<len(out): out[r*w+c] += ms[tr*tw+i] * ns[tw*i+tc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a35511ec-8002-4869-9ebf-c29faf3770a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_2d(m,n, tw=16):\n",
    "    h,k = m.shape\n",
    "    k2,w = n.shape\n",
    "    assert k==k2, 'Size mismatch !'\n",
    "    output = torch.zeros(h,w, dtype=m.dtype)\n",
    "    tpb = dim3(tw,tw)\n",
    "    blocks = dim3(cdiv(w, tpb.x), cdiv(h,tpb.y))\n",
    "    blk_kernel2D_shrd(matmul_tiled_bk, blocks, tpb, tw*tw*2,\n",
    "    #blk_kernel2D_shrd(tiled_matmal_blk, blocks, tpb, tw*tw*2,\n",
    "                m.flatten(), n.flatten(), output.flatten(),\n",
    "                h,w,k, tw=tw)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0afe69c7-5703-4de3-924a-92316ad5f0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 256]), torch.Size([256, 4]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1s.shape, m2s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7f7bdbbd-4438-47e6-b1b5-e94b76aae886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(matmul_2d_(m1s,m2s,tw=16), m1s@m2s).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3203bcaa-ea03-45c7-b799-84f7676e4b9c",
   "metadata": {},
   "source": [
    "### Python threads refactoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d97bed56-48e0-4af7-b947-9e23f9278e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_threads(f, blockDim, *args, **kwargs):\n",
    "    for i0 in range(blockDim.y):\n",
    "        for i1 in range(blockDim.x): f(i0,i1, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7deb7a0d-c166-408c-a138-132a9edad1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiled_matmul2D_blk(blockIdx, blockDim, shared, m, n, out, h, w, k, tw):\n",
    "    shar_sz = tw*tw\n",
    "    ms,ns = shared[:shar_sz],shared[shar_sz:]\n",
    "    \n",
    "    def get_rc(tr,tc): return blockIdx.y*blockDim.y + tr, blockIdx.x*blockDim.x + tc\n",
    "    \n",
    "    def filled_shareMem(tr, tc, ph):\n",
    "        r,c= get_rc(tr,tc)\n",
    "        \n",
    "        ms[tr*tw+tc] = m[tc + ph*tw + r*k] if r<h and (ph*tw+tc)<k else 0.\n",
    "        ns[tr*tw+tc] = n[(tr + ph*tw)*w + c] if c<w and (ph*tw +tr)<k else 0.\n",
    "        \n",
    "    def dotprod_tld(tr,tc):\n",
    "        r,c = get_rc(tr,tc)\n",
    "        for i in range(tw):\n",
    "            if r*w+c<len(out): out[r*w+c] += ms[tr*tw+i] * ns[tw*i+tc]\n",
    "            \n",
    "    for ph in range(int(math.ceil(k/tw))):\n",
    "        run_threads(filled_shareMem,blockDim, ph)\n",
    "        run_threads(dotprod_tld, blockDim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c619da0b-1069-42d0-874a-5bfdd2bee619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_2d(m,n, tw=16):\n",
    "    h,k = m.shape\n",
    "    k2,w = n.shape\n",
    "    assert k==k2, 'Size mismatch !'\n",
    "    output = torch.zeros(h,w, dtype=m.dtype)\n",
    "    tpb = dim3(tw,tw)\n",
    "    blocks = dim3(cdiv(w, tpb.x), cdiv(h,tpb.y))\n",
    "    blk_kernel2D_shrd(tiled_matmul2D_blk, blocks, tpb, tw*tw*2,\n",
    "                m.flatten(), n.flatten(), output.flatten(),\n",
    "                h,w,k, tw=tw)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "eb3792d7-bf78-489c-8d0e-bd5769212523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(matmul_2d(m1s,m2s,tw=16), m1s@m2s).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb292b8-2496-4ef4-be63-ca4cffcae7ba",
   "metadata": {},
   "source": [
    "### barrier sync with python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9bd0de6a-18b4-423e-8c33-570a2285eb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from threading import Barrier, Thread\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "06c53dc3-2cbe-4b69-9f09-ec8ba246e908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x,sb):\n",
    "    print(x)\n",
    "    sb.wait()\n",
    "    print(-x)\n",
    "    sb.wait()\n",
    "    print(x*10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "92902fa7-cff9-428d-aabd-a4ac94f3a8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "-3\n",
      "-1\n",
      "-2\n",
      "10\n",
      "30\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "num =3\n",
    "sb = Barrier(num)\n",
    "with ThreadPoolExecutor(num) as ex: list(ex.map(lambda i: g(i,sb), range(1,num+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7c0cda4e-a888-4f97-aa80-4bec66b0996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blk_kernel_shared(f, blocks,tpb, shr_sz, *args, **kwargs):\n",
    "    for i0 in range(blocks.y):\n",
    "        for i1 in range(blocks.x):\n",
    "            shar = torch.zeros(shr_sz)\n",
    "            syncba = Barrier(tpb.y*tpb.x)\n",
    "            threads = [Thread(target= f, args=(dim3(i1,i0), dim3(p,o), tpb, shar, syncba, *args),kwargs= kwargs)\n",
    "                       for o in range(tpb.y) for p in range(tpb.x)]\n",
    "            for tr in threads: tr.start()\n",
    "            for tr in threads: tr.join()\n",
    "                                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c3a40fc4-8bea-4155-9ab4-3f27f4cd4559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_tiled_bk(blockIdx, threadIdx, blockDim, shared, syncba, m, n, out, h, w, k, tw):\n",
    "    tc,tr = threadIdx.x,threadIdx.y\n",
    "    r = blockIdx.y*blockDim.y + threadIdx.y\n",
    "    c = blockIdx.x*blockDim.x + threadIdx.x\n",
    "    \n",
    "    shar_sz = tw*tw\n",
    "    ms,ns = shared[:shar_sz],shared[shar_sz:]\n",
    "    \n",
    "    p = 0.\n",
    "    for ph in range(cdiv(k,w)):\n",
    "        ms[tr*tw+tc] = m[tc + ph*tw + r*k] if r<h and (ph*tw+tc)<k else 0.\n",
    "        ns[tr*tw+tc] = n[(tr + ph*tw)*w + c] if c<w and (ph*tw +tr)<k else 0.\n",
    "        syncba.wait()\n",
    "        for i in range(tw): p += ms[tr*tw+i] * ns[tw*i+tc]\n",
    "        syncba.wait()\n",
    "        \n",
    "        if(r<h and c<w):  out[r*w+c] =p\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "296cfa2c-c4c7-4b19-92e5-e3654d321619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_2d(m,n, tw=16):\n",
    "    h,k = m.shape\n",
    "    k2,w = n.shape\n",
    "    assert k==k2, 'Size mismatch !'\n",
    "    output = torch.zeros(h,w, dtype=m.dtype)\n",
    "    tpb = dim3(tw,tw)\n",
    "    blocks = dim3(cdiv(w, tpb.x), cdiv(h,tpb.y))\n",
    "    blk_kernel_shared(matmul_tiled_bk, blocks, tpb, tw*tw*2,\n",
    "                m.flatten(), n.flatten(), output.flatten(),\n",
    "                h,w,k, tw=tw)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cd5a126f-63b9-4430-a2a6-611dd28b8a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(matmul_2d(m1s,m2s,tw=8), m1s@m2s).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "431e8e40-3964-47a1-8361-82ba1fd046ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/wurlitzer.py:211: RuntimeWarning: Failed to set pipe buffer size: [Errno 1] Operation not permitted\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cuda_src = cuda_begin + r'''\n",
    "__global__ void matmul_k(float *m, float *n, float *out, int h, int w, int k, int tw){\n",
    "    int tc = threadIdx.x, int tr = threadIdx.y;\n",
    "    int r = blockIdx.y*blockDim.y+tr, int c = blockIdx.x*blockDim.x+tc;\n",
    "    \n",
    "    extern __shared__ float ms[];\n",
    "    float *ns = &ms[tw*tw];\n",
    "    \n",
    "    float p= 0.0f;\n",
    "    for(int ph = 0; ph < cdiv(k,tw); ++ph){\n",
    "        int idx = ph*tw\n",
    "        ms[tr*tw+tc] = r<h && idx+tc<k? m[tc + idx + r*k]: 0.0f;\n",
    "        ms[tr*tw + tc] = c<w && idx+tr<k ? m[ (tr+idx)*w + c] : 0.0f;\n",
    "        __syncthreads();\n",
    "        for (int i=0;i<tw;++i) p += ms[tr*tw+i] * ns[tw*i+ tc];\n",
    "        __syncthreads();\n",
    "        \n",
    "    }\n",
    "    if (r<h && c<w) out[r*w+c] =p;\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aa46f9-437b-4053-af88-55fb8a928cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
